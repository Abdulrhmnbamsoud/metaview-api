{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª"
      ],
      "metadata": {
        "id": "XEpMudxNiqI6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y4CktadiQj5"
      },
      "outputs": [],
      "source": [
        "!pip install feedparser beautifulsoup4 requests html5lib\n",
        "!pip install sentence-transformers faiss-cpu\n",
        "!pip install transformers accelerate\n",
        "!pip install fastapi uvicorn nest_asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "import requests\n",
        "import feedparser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "n4by4p9uifl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¹Ø±ÙŠÙ Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ø®Ø¨Ø§Ø± (RSS)"
      ],
      "metadata": {
        "id": "iq7wPo91il4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCES = [\n",
        "    # id, rss_url\n",
        "    (\"reuters_world\", \"http://feeds.reuters.com/Reuters/worldNews\"),\n",
        "    (\"nyt_world\", \"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\"),\n",
        "    (\"wsj_world\", \"https://feeds.a.dj.com/rss/RSSWorldNews.xml\"),\n",
        "    (\"economist_international\", \"https://www.economist.com/international/rss.xml\"),\n",
        "    (\"cnn_world\", \"http://rss.cnn.com/rss/cnn_world.rss\"),\n",
        "    (\"bbc_world\", \"http://feeds.bbci.co.uk/news/world/rss.xml\"),\n",
        "    (\"dw_world\", \"https://rss.dw.com/rdf/rss-en-world\"),\n",
        "    (\"france24_en\", \"https://www.france24.com/en/rss\"),\n",
        "    (\"guardian_world\", \"https://www.theguardian.com/world/rss\"),\n",
        "    (\"washington_post_world\", \"http://feeds.washingtonpost.com/rss/world\"),\n",
        "    (\"ap_top\", \"https://apnews.com/apf-topnews?output=rss\"),\n",
        "    (\"axios\", \"https://www.axios.com/feeds/feed.rss\"),\n",
        "    (\"the_hill\", \"https://thehill.com/feed/\"),\n",
        "    (\"newsweek\", \"https://www.newsweek.com/rss\"),\n",
        "    (\"alarabiya_en\", \"https://english.alarabiya.net/.mrss/en.xml\"),\n",
        "    (\"cbs_world\", \"https://www.cbsnews.com/latest/rss/world\"),\n",
        "    (\"nbc_world\", \"http://feeds.nbcnews.com/feeds/worldnews\"),\n",
        "    (\"abc_world\", \"http://feeds.abcnews.com/abcnews/internationalheadlines\"),\n",
        "    (\"ft_world\", \"https://www.ft.com/world/rss\"),\n",
        "]\n"
      ],
      "metadata": {
        "id": "8xW6VJNNijVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø³ÙƒØ±Ø¨ÙŠÙ†Ù‚ RSS"
      ],
      "metadata": {
        "id": "l5Wxzageiyi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
        "}\n",
        "\n",
        "def fetch_rss(source_id: str, url: str, timeout: int = 10) -> List[Dict]:\n",
        "    out = []\n",
        "    try:\n",
        "        feed = feedparser.parse(url)\n",
        "        for entry in feed.entries:\n",
        "            link = entry.get(\"link\", \"\")\n",
        "            title = entry.get(\"title\", \"\").strip()\n",
        "            summary = entry.get(\"summary\", \"\").strip()\n",
        "            published = entry.get(\"published\", \"\") or entry.get(\"updated\", \"\")\n",
        "\n",
        "            out.append({\n",
        "                \"source\": source_id,\n",
        "                \"headline\": title,\n",
        "                \"summary\": summary,\n",
        "                \"link\": link,\n",
        "                \"published_raw\": published,\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"[RSS] {source_id} error:\", e)\n",
        "    return out\n",
        "\n",
        "all_rows = []\n",
        "for sid, url in SOURCES:\n",
        "    print(\">>\", sid)\n",
        "    rows = fetch_rss(sid, url)\n",
        "    print(\"  grabbed:\", len(rows))\n",
        "    all_rows.extend(rows)\n",
        "\n",
        "df_rss = pd.DataFrame(all_rows).drop_duplicates(subset=[\"source\", \"headline\", \"link\"])\n",
        "df_rss.head()\n"
      ],
      "metadata": {
        "id": "kchxbuRtiv8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªØ§Ø±ÙŠØ® + Ø³ÙƒØ±Ø¨ÙŠÙ†Ù‚ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ù„"
      ],
      "metadata": {
        "id": "1FpbTyWRi21r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_date_safe(x: str) -> Optional[datetime]:\n",
        "    try:\n",
        "        return pd.to_datetime(x)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "df_rss[\"published_at\"] = df_rss[\"published_raw\"].apply(parse_date_safe)\n",
        "\n",
        "def extract_article_text(url: str, timeout: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Ø³ÙƒØ±Ø¨ÙŠÙ†Ù‚ Ø¨Ø³ÙŠØ· Ù„Ù„Ù…Ù‚Ø§Ù„. Ù…Ùˆ ÙƒØ§Ù…Ù„ 100% Ø¨Ø³ ÙŠØ¹Ø·ÙŠÙƒ Ù…Ø­ØªÙˆÙ‰ ÙƒÙØ§ÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„.\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "        if resp.status_code >= 400:\n",
        "            return \"\"\n",
        "\n",
        "        html = resp.text\n",
        "        soup = BeautifulSoup(html, \"html5lib\")\n",
        "\n",
        "        # Ø¬Ø±Ø¨ Ù†Ù„Ù‚Ø· <article> Ø¥Ø°Ø§ Ù…ÙˆØ¬ÙˆØ¯\n",
        "        article_tag = soup.find(\"article\")\n",
        "        if article_tag:\n",
        "            texts = [p.get_text(\" \", strip=True) for p in article_tag.find_all([\"p\", \"h2\", \"li\"])]\n",
        "            content = \" \".join(texts)\n",
        "            if len(content) > 300:\n",
        "                return content\n",
        "\n",
        "        # fallback: divs Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\n",
        "        candidates = []\n",
        "        for cls in [\"article-body\", \"story-body\", \"post-content\", \"entry-content\", \"c-article-body\"]:\n",
        "            div = soup.find(\"div\", class_=lambda x: x and cls in x)\n",
        "            if div:\n",
        "                candidates.append(div)\n",
        "\n",
        "        for div in candidates:\n",
        "            texts = [p.get_text(\" \", strip=True) for p in div.find_all([\"p\", \"h2\", \"li\"])]\n",
        "            content = \" \".join(texts)\n",
        "            if len(content) > 300:\n",
        "                return content\n",
        "\n",
        "        # fallback Ø£Ø®ÙŠØ±: ÙƒÙ„ Ø§Ù„ÙÙ‚Ø±Ø§Øª\n",
        "        texts = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "        content = \" \".join(texts)\n",
        "        return content\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "# Ø³ÙƒØ±Ø¨ÙŠÙ†Ù‚ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª (Ù…Ù…ÙƒÙ† ØªØ³ØªØºØ±Ù‚ Ø´ÙˆÙŠ)\n",
        "contents = []\n",
        "for i, row in df_rss.iterrows():\n",
        "    if i % 50 == 0:\n",
        "        print(\"article\", i, \"/\", len(df_rss))\n",
        "    contents.append(extract_article_text(row[\"link\"]))\n",
        "\n",
        "df_rss[\"content\"] = contents\n",
        "df_rss[\"content\"] = df_rss[\"content\"].fillna(\"\").astype(str)\n",
        "\n",
        "df_rss.to_csv(\"news_raw_with_content.csv\", index=False)\n",
        "len(df_rss)\n"
      ],
      "metadata": {
        "id": "UcPtthGQiv6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Article Scraping"
      ],
      "metadata": {
        "id": "-YY-a7dmoAZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "import pandas as pd\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\"\n",
        "}\n",
        "\n",
        "def extract_article_text(url: str, timeout: int = 10) -> str:\n",
        "    \"\"\"\n",
        "    Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ© Ù…Ù† Ø§Ù„Ù€ Hybrid Architecture:\n",
        "    RSS = Ù†Ù‚Ø·Ø© Ø¯Ø®ÙˆÙ„ Ø±Ø³Ù…ÙŠØ©\n",
        "    Content Scraping = Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø§Ù„Ù†ØµÙŠ Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±\n",
        "    \"\"\"\n",
        "    if not url:\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=timeout)\n",
        "        if resp.status_code >= 400:\n",
        "            return \"\"\n",
        "\n",
        "        soup = BeautifulSoup(resp.text, \"html5lib\")\n",
        "\n",
        "        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªÙ‚Ø§Ø· article tag Ø¥Ù† ÙˆØ¬Ø¯\n",
        "        article = soup.find(\"article\")\n",
        "        if article:\n",
        "            parts = [p.get_text(\" \", strip=True) for p in article.find_all([\"p\", \"h2\", \"li\"])]\n",
        "            text = \" \".join(parts)\n",
        "            if len(text) > 300:\n",
        "                return text\n",
        "\n",
        "        # Ø¨Ø¯Ø§Ø¦Ù„ Ø´Ø§Ø¦Ø¹Ø© Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ù„\n",
        "        candidates = [\n",
        "            \"article-body\", \"story-body\", \"entry-content\",\n",
        "            \"post-content\", \"c-article-body\", \"article__content\"\n",
        "        ]\n",
        "\n",
        "        for cls in candidates:\n",
        "            div = soup.find(\"div\", class_=lambda x: x and cls in x)\n",
        "            if div:\n",
        "                parts = [p.get_text(\" \", strip=True) for p in div.find_all([\"p\", \"h2\", \"li\"])]\n",
        "                text = \" \".join(parts)\n",
        "                if len(text) > 300:\n",
        "                    return text\n",
        "\n",
        "\n",
        "        parts = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\")]\n",
        "        return \" \".join(parts)\n",
        "\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def scrape_one(item):\n",
        "    idx, url = item\n",
        "    text = extract_article_text(url)\n",
        "    return idx, text\n",
        "\n",
        "urls = [(i, row[\"link\"]) for i, row in df_rss.iterrows()]\n",
        "\n",
        "contents = [\"\"] * len(df_rss)\n",
        "\n",
        "print(\"Starting threaded scraping...\")\n",
        "done = 0\n",
        "total = len(urls)\n",
        "\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
        "    for idx, text in executor.map(scrape_one, urls):\n",
        "        contents[idx] = text\n",
        "        done += 1\n",
        "        if done % 50 == 0:\n",
        "            print(f\"Processed {done}/{total}\")\n",
        "\n",
        "df_rss[\"content\"] = contents\n",
        "df_rss[\"content\"] = df_rss[\"content\"].fillna(\"\").astype(str)\n",
        "\n",
        "df_rss.to_csv(\"news_raw_with_content.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Done â€” enriched articles saved.\")\n",
        "len(df_rss)\n"
      ],
      "metadata": {
        "id": "nGRxPnjHn9pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ØªØµÙ†ÙŠÙ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø®Ø¨Ø± + Ø§Ù„Ø¯ÙˆÙ„Ø©"
      ],
      "metadata": {
        "id": "cFGN6eXni8xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_rss.copy()\n",
        "\n",
        "df[\"full_text\"] = (df[\"headline\"].fillna(\"\") + \" \" + df[\"content\"].fillna(\"\")).astype(str)\n",
        "\n",
        "DOMAIN_KEYWORDS = {\n",
        "    \"Security\": [\n",
        "        \"war\",\"conflict\",\"military\",\"attack\",\"terror\",\"missile\",\"strike\",\"army\",\n",
        "        \"Ø­Ø±Ø¨\",\"Ù‡Ø¬ÙˆÙ…\",\"ØµØ§Ø±ÙˆØ®\",\"Ø£Ù…Ù†\",\"ØªÙØ¬ÙŠØ±\",\"Ù…ÙŠÙ„ÙŠØ´ÙŠØ§\"\n",
        "    ],\n",
        "    \"Economy\": [\n",
        "        \"economy\",\"economic\",\"inflation\",\"interest rate\",\"market\",\"stock\",\"debt\",\n",
        "        \"Ø§Ù‚ØªØµØ§Ø¯\",\"ØªØ¶Ø®Ù…\",\"Ø¨Ø·Ø§Ù„Ø©\",\"Ø³ÙˆÙ‚\",\"Ø¨ÙˆØ±ØµØ©\",\"Ù†Ù…Ùˆ\",\"Ù…ÙŠØ²Ø§Ù†ÙŠØ©\"\n",
        "    ],\n",
        "    \"Energy\": [\n",
        "        \"oil\",\"gas\",\"energy\",\"OPEC\",\"fuel\",\"power plant\",\n",
        "        \"Ù†ÙØ·\",\"ØºØ§Ø²\",\"Ø·Ø§Ù‚Ø©\",\"Ø¨Ø±Ù…ÙŠÙ„\",\"Ø£ÙˆØ¨Ùƒ\"\n",
        "    ],\n",
        "    \"Politics\": [\n",
        "        \"president\",\"prime minister\",\"election\",\"government\",\"parliament\",\"sanction\",\n",
        "        \"Ø±Ø¦ÙŠØ³\",\"Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª\",\"Ø­ÙƒÙˆÙ…Ø©\",\"Ø¹Ù‚ÙˆØ¨Ø§Øª\",\"Ø¨Ø±Ù„Ù…Ø§Ù†\",\"Ø¯Ø¨Ù„ÙˆÙ…Ø§Ø³ÙŠØ©\"\n",
        "    ],\n",
        "    \"Tech\": [\n",
        "        \"technology\",\"AI\",\"cyber\",\"data\",\"software\",\"chip\",\"semiconductor\",\n",
        "        \"ØªÙ‚Ù†ÙŠØ©\",\"Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\",\"Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ\",\"Ø¨ÙŠØ§Ù†Ø§Øª\",\"Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "def classify_domain(text: str) -> str:\n",
        "    t = text.lower()\n",
        "    scores = {k: 0 for k in DOMAIN_KEYWORDS}\n",
        "    for dom, kws in DOMAIN_KEYWORDS.items():\n",
        "        for kw in kws:\n",
        "            if kw.lower() in t:\n",
        "                scores[dom] += 1\n",
        "    best = max(scores, key=scores.get)\n",
        "    return best if scores[best] > 0 else \"Other\"\n",
        "\n",
        "df[\"domain\"] = df[\"full_text\"].apply(classify_domain)\n",
        "\n",
        "# Ø®Ø±ÙŠØ·Ø© Ø¯ÙˆÙ„ Ø¨Ø³ÙŠØ·Ø©\n",
        "COUNTRY_MAP = {\n",
        "    \"Saudi Arabia\": [\"saudi arabia\",\"riyadh\",\"Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\",\"Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©\"],\n",
        "    \"United States\": [\"united states\",\"u.s.\",\"usa\",\"washington\"],\n",
        "    \"United Kingdom\": [\"united kingdom\",\"uk\",\"london\"],\n",
        "    \"France\": [\"france\",\"paris\"],\n",
        "    \"Germany\": [\"germany\",\"berlin\"],\n",
        "    \"Russia\": [\"russia\",\"moscow\"],\n",
        "    \"China\": [\"china\",\"beijing\"],\n",
        "    \"Iran\": [\"iran\",\"tehran\",\"Ø¥ÙŠØ±Ø§Ù†\"],\n",
        "    \"Israel\": [\"israel\",\"ØªÙ„ Ø£Ø¨ÙŠØ¨\",\"Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„\"],\n",
        "}\n",
        "\n",
        "def extract_country(text: str) -> str:\n",
        "    low = text.lower()\n",
        "    for country, kws in COUNTRY_MAP.items():\n",
        "        for kw in kws:\n",
        "            if kw.lower() in low:\n",
        "                return country\n",
        "    return \"\"\n",
        "\n",
        "df[\"country\"] = df[\"full_text\"].apply(extract_country)\n",
        "\n",
        "df.to_csv(\"news_enriched_step1.csv\", index=False)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "zDP1IFLniv3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø±"
      ],
      "metadata": {
        "id": "kH9RKXnGjGYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "def safe_sentiment(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "    if len(text) < 30:\n",
        "        return (\"NEUTRAL\", 0.0)\n",
        "    out = sentiment_model(text[:512])[0]\n",
        "    return (out[\"label\"], float(out[\"score\"]))\n",
        "\n",
        "labels, scores = [], []\n",
        "for i, txt in enumerate(df[\"content\"].tolist()):\n",
        "    if i % 50 == 0:\n",
        "        print(\"sent\", i, \"/\", len(df))\n",
        "    lab, sc = safe_sentiment(txt)\n",
        "    labels.append(lab)\n",
        "    scores.append(sc)\n",
        "\n",
        "df[\"sentiment_label\"] = labels\n",
        "df[\"sentiment_score\"] = scores\n",
        "\n",
        "df.to_csv(\"news_with_sentiment.csv\", index=False)\n",
        "len(df)\n"
      ],
      "metadata": {
        "id": "3CuNK3KWjGKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings + FAISS Index"
      ],
      "metadata": {
        "id": "OG88TF0GjKFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "texts = df[\"full_text\"].fillna(\"\").tolist()\n",
        "emb = embed_model.encode(\n",
        "    texts,\n",
        "    batch_size=64,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "# normalize\n",
        "emb = emb / (np.linalg.norm(emb, axis=1, keepdims=True) + 1e-10)\n",
        "\n",
        "dim = emb.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "index.add(emb)\n",
        "\n",
        "faiss.write_index(index, \"news_faiss_index.bin\")\n",
        "df.to_parquet(\"news_final.parquet\", index=False)\n",
        "\n",
        "print(\"index size:\", index.ntotal)\n"
      ],
      "metadata": {
        "id": "ceX6Fn0DjGIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ ÙƒÙÙ†ÙƒØ´Ù†"
      ],
      "metadata": {
        "id": "jAgZeO7RjMST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"news_final.parquet\")\n",
        "index = faiss.read_index(\"news_faiss_index.bin\")\n",
        "\n",
        "def semantic_search(query: str,\n",
        "                    top_k: int = 20,\n",
        "                    domain: Optional[str] = None,\n",
        "                    country: Optional[str] = None) -> pd.DataFrame:\n",
        "    if not query.strip():\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True)[0]\n",
        "    q_emb = q_emb / (np.linalg.norm(q_emb) + 1e-10)\n",
        "\n",
        "    D, I = index.search(q_emb.reshape(1, -1), top_k * 3)\n",
        "    scores, idxs = D[0], I[0]\n",
        "\n",
        "    res = df.iloc[idxs].copy()\n",
        "    res[\"semantic_score\"] = scores\n",
        "\n",
        "    if domain:\n",
        "        res = res[res[\"domain\"] == domain]\n",
        "\n",
        "    if country:\n",
        "        res = res[res[\"country\"] == country]\n",
        "\n",
        "    res = res.sort_values(\"semantic_score\", ascending=False).head(top_k)\n",
        "\n",
        "    return res[[\n",
        "        \"semantic_score\",\n",
        "        \"published_at\",\n",
        "        \"source\",\n",
        "        \"domain\",\n",
        "        \"country\",\n",
        "        \"headline\",\n",
        "        \"summary\",\n",
        "        \"sentiment_label\",\n",
        "        \"sentiment_score\",\n",
        "        \"link\"\n",
        "    ]]\n",
        "\n",
        "# ØªØ¬Ø±Ø¨Ø© Ø³Ø±ÙŠØ¹Ø©\n",
        "semantic_search(\"Saudi oil production\", top_k=5)\n"
      ],
      "metadata": {
        "id": "peSv0y8Tiv1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Narrative Divergence Engine"
      ],
      "metadata": {
        "id": "WoL7EINdjTxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def narrative_divergence(query: str,\n",
        "                         top_k: int = 200,\n",
        "                         min_articles_per_country: int = 3):\n",
        "    \"\"\"\n",
        "    ÙŠÙ‚ÙŠØ³ Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø³Ø±Ø¯ Ø¨ÙŠÙ† Ø§Ù„Ø¯ÙˆÙ„ Ù„Ù†ÙØ³ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹.\n",
        "    ÙŠØ±Ø¬Ù‘Ø¹: divergence_index, summary_df\n",
        "    \"\"\"\n",
        "    base_res = semantic_search(query, top_k=top_k)\n",
        "    if base_res.empty:\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    # Ù†Ø­ØªØ§Ø¬ ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© + embeddings Ù…Ù† Ø¬Ø¯ÙŠØ¯\n",
        "    idxs = base_res.index.tolist()\n",
        "    tmp = df.loc[idxs].copy()\n",
        "    tmp = tmp[tmp[\"country\"].astype(str) != \"\"]\n",
        "    if tmp.empty:\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    texts = (tmp[\"headline\"].fillna(\"\") + \" \" + tmp[\"content\"].fillna(\"\")).tolist()\n",
        "    emb_local = embed_model.encode(texts, convert_to_numpy=True)\n",
        "    emb_local = emb_local / (np.linalg.norm(emb_local, axis=1, keepdims=True) + 1e-10)\n",
        "\n",
        "    tmp = tmp.reset_index(drop=True)\n",
        "\n",
        "    country_centroids = {}\n",
        "    country_counts = {}\n",
        "    for country in tmp[\"country\"].unique():\n",
        "        idx_c = tmp[tmp[\"country\"] == country].index.values\n",
        "        if len(idx_c) < min_articles_per_country:\n",
        "            continue\n",
        "        vecs = emb_local[idx_c]\n",
        "        c = vecs.mean(axis=0)\n",
        "        c = c / (np.linalg.norm(c) + 1e-10)\n",
        "        country_centroids[country] = c\n",
        "        country_counts[country] = len(idx_c)\n",
        "\n",
        "    if len(country_centroids) < 2:\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    countries = list(country_centroids.keys())\n",
        "    centroids = np.stack([country_centroids[c] for c in countries], axis=0)\n",
        "\n",
        "    dists = []\n",
        "    for i in range(len(countries)):\n",
        "        for j in range(i+1, len(countries)):\n",
        "            v1, v2 = centroids[i], centroids[j]\n",
        "            cos_sim = float(np.dot(v1, v2))\n",
        "            dists.append(1.0 - cos_sim)\n",
        "\n",
        "    div_index = float(np.mean(dists))\n",
        "\n",
        "    # summary table\n",
        "    avg_sents = []\n",
        "    for c in countries:\n",
        "        s = tmp[tmp[\"country\"] == c][\"sentiment_score\"].dropna()\n",
        "        avg_sents.append(s.mean() if not s.empty else np.nan)\n",
        "\n",
        "    summary_df = pd.DataFrame({\n",
        "        \"country\": countries,\n",
        "        \"article_count\": [country_counts[c] for c in countries],\n",
        "        \"avg_sentiment\": avg_sents\n",
        "    }).sort_values(\"article_count\", ascending=False)\n",
        "\n",
        "    return div_index, summary_df\n",
        "\n",
        "# ØªØ¬Ø±Ø¨Ø©\n",
        "div_idx, div_table = narrative_divergence(\"Gaza war\", top_k=150)\n",
        "div_idx, div_table.head()\n"
      ],
      "metadata": {
        "id": "Y2sa0zH-jUHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastAPI"
      ],
      "metadata": {
        "id": "ZKyWoPJIjVb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ğŸ”¹ Step 12 â€” Define News Intelligence API (FastAPI)\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"NARRA â€“ News Intelligence API\",\n",
        "    description=(\n",
        "        \"Backend for global news intelligence: semantic search, \"\n",
        "        \"sentiment, and narrative divergence between countries.\"\n",
        "    ),\n",
        "    version=\"1.0.0\",\n",
        ")\n",
        "\n",
        "# ---------- Request models ----------\n",
        "\n",
        "class SearchRequest(BaseModel):\n",
        "    query: str\n",
        "    top_k: int = 20\n",
        "    domain: Optional[str] = None\n",
        "    country: Optional[str] = None\n",
        "\n",
        "\n",
        "class DivergenceRequest(BaseModel):\n",
        "    query: str\n",
        "    top_k: int = 200\n",
        "\n",
        "\n",
        "# ---------- Health check ----------\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    \"\"\"\n",
        "    Simple health check to make sure the API is up.\n",
        "    \"\"\"\n",
        "    return {\"status\": \"ok\", \"service\": \"news_intelligence_api\"}\n",
        "\n",
        "\n",
        "# ---------- Semantic search endpoint ----------\n",
        "\n",
        "@app.post(\"/semantic-search\")\n",
        "def api_semantic_search(body: SearchRequest):\n",
        "    \"\"\"\n",
        "    Semantic search over news articles using FAISS + sentence embeddings.\n",
        "    \"\"\"\n",
        "    res = semantic_search(\n",
        "        query=body.query,\n",
        "        top_k=body.top_k,\n",
        "        domain=body.domain,\n",
        "        country=body.country,\n",
        "    )\n",
        "\n",
        "    # Make sure we always return a JSON-friendly structure\n",
        "    results = []\n",
        "    if not res.empty:\n",
        "        results = res.to_dict(orient=\"records\")\n",
        "\n",
        "    return {\n",
        "        \"query\": body.query,\n",
        "        \"top_k\": body.top_k,\n",
        "        \"count\": len(results),\n",
        "        \"results\": results,\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------- Narrative divergence endpoint ----------\n",
        "\n",
        "@app.post(\"/narrative-divergence\")\n",
        "def api_narrative_divergence(body: DivergenceRequest):\n",
        "    \"\"\"\n",
        "    Compute narrative divergence index between countries\n",
        "    for a given topic/query.\n",
        "    \"\"\"\n",
        "    div_idx, table = narrative_divergence(\n",
        "        query=body.query,\n",
        "        top_k=body.top_k,\n",
        "    )\n",
        "\n",
        "    if div_idx is None or table is None or table.empty:\n",
        "        return {\n",
        "            \"query\": body.query,\n",
        "            \"top_k\": body.top_k,\n",
        "            \"divergence_index\": None,\n",
        "            \"detail\": [],\n",
        "            \"message\": \"Not enough data to compute divergence (countries/articles too few).\",\n",
        "        }\n",
        "\n",
        "    detail = table.to_dict(orient=\"records\")\n",
        "\n",
        "    return {\n",
        "        \"query\": body.query,\n",
        "        \"top_k\": body.top_k,\n",
        "        \"divergence_index\": float(div_idx),\n",
        "        \"detail\": detail,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "sPNd5oOU7fBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the API server inside Colab"
      ],
      "metadata": {
        "id": "weBSaDi67meI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ğŸ”¹ Step 13 â€” Run FastAPI server inside Colab\n",
        "\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "# Allow running uvicorn inside Jupyter/Colab event loop\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the server (blocking cell â€“ keep it last)\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "cXkp3eYa7mP1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}